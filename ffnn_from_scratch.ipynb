{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2678df29",
   "metadata": {},
   "source": [
    "# Building feed forward NN from scratch using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a40d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value: np.ndarray, grad: np.ndarray = None):\n",
    "        # value computed in the forward pass\n",
    "        self.value = value\n",
    "        # derivative of circuit, computed in backward pass\n",
    "        self.grad = grad\n",
    "        if self.grad is None:\n",
    "            self.grad = np.zeros_like(self.value)\n",
    "        assert self.value.shape == self.grad.shape\n",
    "\n",
    "\n",
    "# Applies a linear transformation to the incoming datasets: y = xW + b, coefficients W and b are differentiated\n",
    "class LayerLinear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # initialize coefficients with real numbers from 0..1\n",
    "        self.w: Variable = Variable(np.random.uniform(size=(in_features, out_features)))\n",
    "        self.b: Variable = Variable(np.random.uniform(size=out_features))\n",
    "        self.x: Variable = None\n",
    "        self.out: Variable = None\n",
    "\n",
    "    def forward(self, x: Variable):\n",
    "        self.x = x\n",
    "        # matmul adds one more dimension, if one of arrays is one dimensional\n",
    "        # (see https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html)\n",
    "        self.out = Variable(np.matmul(self.x.value, self.w.value) + self.b.value)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        # w.grad = x.val x out.grad =>  (100,2,40) = (100,2,1) x (100,1,40)\n",
    "        self.w.grad = np.matmul(np.expand_dims(self.x.value, axis=2), np.expand_dims(self.out.grad, axis=1))\n",
    "        # w.grad => average to (2,40)\n",
    "        self.w.grad = np.average(self.w.grad, axis=0)\n",
    "        # b.grad => average from 1 x (100,40) to (1,40)\n",
    "        self.b.grad = np.average(self.out.grad, axis=0)\n",
    "        # out.grad need to be resized back with w.val, reverse process to linear transformation\n",
    "        # x.grad = out.grad x w.val^T => (100,2) = (100, 40) x (40,2)\n",
    "        self.x.grad = np.matmul(self.out.grad, np.transpose(self.w.value))\n",
    "\n",
    "    def parameters(self):\n",
    "        # return linear transformation coefficients for step update\n",
    "        l = list()\n",
    "        l.append(self.w)\n",
    "        l.append(self.b)\n",
    "        return l\n",
    "\n",
    "\n",
    "# Applies a sigmoid function y(x) = 1 / (1 + e^(-x))\n",
    "class LayerSigmoid:\n",
    "    def __init__(self):\n",
    "        self.x: Variable = None\n",
    "        self.out: Variable = None\n",
    "\n",
    "    def forward(self, x: Variable):\n",
    "        self.x = x\n",
    "        self.out = Variable(1 / (1 + np.exp(-self.x.value)))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        # derivative of sigmoid(x) = sigmoid(x) * (1 - sigmoid(x))\n",
    "        self.x.grad = self.out.value * (1 - self.out.value) * self.out.grad\n",
    "\n",
    "    def parameters(self):\n",
    "        # return empty list, since no coefficients\n",
    "        return list()\n",
    "\n",
    "\n",
    "# Applies a ReLU(Rectified Linear Unit) function\n",
    "# ReLU(x)=max(0,x)\n",
    "class LayerReLU:\n",
    "    def __init__(self):\n",
    "        self.x: Variable = None\n",
    "        self.out: Variable = None\n",
    "\n",
    "    def forward(self, x: Variable):\n",
    "        self.x = x\n",
    "        self.out = Variable(self.x.value * (self.x.value > 0))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        # derivative of ReLU(x) = 1 if x is positive and 0 if not\n",
    "        self.x.grad = (self.x.value > 0) * self.out.grad\n",
    "\n",
    "    def parameters(self):\n",
    "        # return empty list, since no coefficients\n",
    "        return list()\n",
    "\n",
    "\n",
    "# Applies a Tanh(Hyperbolic tangent) function\n",
    "# Tanh(x)=tanh(x)= (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "class LayerTanh:\n",
    "    def __init__(self):\n",
    "        self.x: Variable = None\n",
    "        self.out: Variable = None\n",
    "        # positive and negative exponents\n",
    "        self.e_p = None\n",
    "        self.e_n = None\n",
    "\n",
    "    def forward(self, x: Variable):\n",
    "        self.x = x\n",
    "        self.e_p = np.exp(self.x.value)\n",
    "        self.e_n = np.exp(-self.x.value)\n",
    "        tanh = (self.e_p - self.e_n) / (self.e_p + self.e_n)\n",
    "        self.out = Variable(tanh)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        # derivative of Tanh(x) is square of hyperbolic secant\n",
    "        # sech(x) = 2 / (e^x + e^(-x))\n",
    "        # check https://www.math24.net/derivatives-hyperbolic-functions/\n",
    "        sech = 2 / (self.e_p + self.e_n)\n",
    "        self.x.grad = sech ** 2 * self.out.grad\n",
    "\n",
    "    def parameters(self):\n",
    "        # return empty list, since no coefficients\n",
    "        return list()\n",
    "\n",
    "\n",
    "# Applies the Softmax function to an n-dimensional input Tensor rescaling them\n",
    "# so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.\n",
    "# shift in forward() is used to minimize calculations <=> dividing all e^x to constant e^D, to calculate only e^x-D\n",
    "class LayerSoftmax:\n",
    "    def __init__(self):\n",
    "        self.x: Variable = None\n",
    "        self.out: Variable = None\n",
    "        self.e_x = None\n",
    "        self.sum = None\n",
    "        self.jacobian = None\n",
    "\n",
    "    def forward(self, x: Variable):\n",
    "        self.x = x\n",
    "        # get max elem of each line\n",
    "        self.e_x = np.exp(self.x.value - np.max(self.x.value, axis=1))\n",
    "        # sum each line <=> second dimension\n",
    "        self.sum = self.e_x.sum(axis=1)\n",
    "        self.sum = np.expand_dims(self.sum, axis=1)\n",
    "        self.out = Variable(self.e_x / self.sum)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        # create Jacobian matrix NxN with softmax derivatives, e.x. (100, 2, 2)\n",
    "        self.jacobian = np.zeros(self.x.grad[0], self.x.grad[1], self.x.grad[1])\n",
    "        # D_i S_j = S_i (1 - S_i) if i = j | - S_j * S_i if i != j;\n",
    "        # check https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        # jacobian[k,i,j]\n",
    "        for k in range(self.jacobian.shape[0]):\n",
    "            for i in range(self.jacobian.shape[1]):\n",
    "                for j in range(self.jacobian.shape[2]):\n",
    "                    if i == j:\n",
    "                        self.jacobian[k][i][j] = self.out[k][i] * (1 - self.out[k][i])\n",
    "                    else:\n",
    "                        self.jacobian[k][i][j] = self.out[k][i] * self.out[k][j]\n",
    "\n",
    "        # (100, 1, 2) = (100, 1, 2) x (100, 2, 2)\n",
    "        self.x.grad = np.matmul(np.exp(self.out.grad, axis=1), self.jacobian)\n",
    "        # average of second axis, to get (100,2)\n",
    "        self.x.grad = np.average(self.x.grad, axis=1)\n",
    "\n",
    "    def parameters(self):\n",
    "        # return empty list, since no coefficients\n",
    "        return list()\n",
    "\n",
    "\n",
    "# Applies a Mean Squared Error estimator\n",
    "class LossMSE:\n",
    "    def __init__(self):\n",
    "        self.y_predicted: Variable = None\n",
    "        self.y_correct: Variable = None\n",
    "        self.out: Variable = None\n",
    "\n",
    "    def forward(self, y_correct: Variable, y_predicted: Variable):\n",
    "        self.y_predicted = y_predicted\n",
    "        self.y_correct = y_correct\n",
    "        self.out = np.mean((y_correct.value - y_predicted.value) ** 2)\n",
    "        self.out = Variable(np.asarray([self.out]))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.y_predicted.grad = 2 * (1 / self.y_predicted.value.shape[0]) * (\n",
    "                self.y_predicted.value - self.y_correct.value)\n",
    "\n",
    "\n",
    "class FeedForwardNeuralNet:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers: list = layers\n",
    "\n",
    "    def forward(self, x: Variable):\n",
    "        out: Variable = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward()\n",
    "\n",
    "    def parameters(self):\n",
    "        parameters = list()\n",
    "        for layer in self.layers:\n",
    "            parameters.append(layer.parameters())\n",
    "        # flatten\n",
    "        flat_list = [item for sublist in parameters for item in sublist]\n",
    "        return flat_list\n",
    "\n",
    "\n",
    "class Optimiser:\n",
    "    def __init__(self, parameters: list, lr):\n",
    "        # list of variables\n",
    "        self.parameters = parameters\n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            # to minimize loss function using gradient descent, need to go down with gradient\n",
    "            p.value -= self.lr * p.grad\n",
    "\n",
    "\n",
    "# coefficient of determination: https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "def r2_score(y_correct: np.ndarray, y_predicted: np.ndarray):\n",
    "    mean_correct = np.mean(y_correct)\n",
    "\n",
    "    # scikit r2_score uses 1 - ss_res / ss_tot\n",
    "    # ss_res = np.sum((y_predicted - y_correct) ** 2)\n",
    "\n",
    "    ss_reg = np.sum((y_predicted - mean_correct) ** 2)\n",
    "    ss_tot = np.sum((y_correct - mean_correct) ** 2)\n",
    "    return 1 - ss_reg / ss_tot\n",
    "\n",
    "\n",
    "'''\n",
    "Batch size better to be 2^n\n",
    "Batch Gradient Descent. Batch Size = Size of Training Set\n",
    "Stochastic Gradient Descent. Batch Size = 1\n",
    "Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Generate the input datasets\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    input_dim = 2\n",
    "    output_dim = 1\n",
    "    train_size = batch_size * 80\n",
    "    test_size = batch_size * 20\n",
    "\n",
    "    x = np.random.random((train_size + test_size, input_dim))\n",
    "    # y = x_1^2 + x_2\n",
    "    y = np.asarray([i[0] ** 2 + i[1] for i in x], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    x_train, x_test = Variable(x[:train_size]), Variable(x[train_size:])\n",
    "    y_train, y_test = Variable(y[:train_size]), Variable(y[train_size:])\n",
    "\n",
    "    # Instantiate network\n",
    "\n",
    "    # layers = [LayerLinear(input_dim, 40),\n",
    "    #           LayerSigmoid(),\n",
    "    #           LayerLinear(40, 20),\n",
    "    #           LayerSigmoid(),\n",
    "    #           LayerLinear(20, output_dim)]\n",
    "\n",
    "    layers = [LayerLinear(input_dim, 40),\n",
    "              LayerReLU(),\n",
    "              LayerLinear(40, 20),\n",
    "              LayerTanh(),\n",
    "              LayerLinear(20, output_dim)]\n",
    "    model = FeedForwardNeuralNet(layers)\n",
    "\n",
    "    # Training\n",
    "    l_rate = 1e-3\n",
    "    loss = LossMSE()\n",
    "    optimiser = Optimiser(model.parameters(), lr=l_rate)\n",
    "\n",
    "    r2_scores = []\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # collect metrics for each epoch (calculate average metrics of each batch)\n",
    "        losses_epoch = []\n",
    "        r2_scores_epoch = []\n",
    "\n",
    "        # training datasets is divided by size of one batch and iterated\n",
    "        for batch in range((train_size - batch_size) // batch_size):\n",
    "            x_train_batch = Variable(x_train.value[(batch * batch_size): ((batch + 1) * batch_size):])\n",
    "            y_train_batch = Variable(y_train.value[(batch * batch_size): ((batch + 1) * batch_size):])\n",
    "\n",
    "            # forward\n",
    "            y_predicted = model.forward(x_train_batch)\n",
    "\n",
    "            # metrics\n",
    "            losses_epoch.append(loss.forward(y_train_batch, y_predicted).value)\n",
    "            r2_scores_epoch.append(r2_score(y_train.value, y_predicted.value))\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            model.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimiser.step()\n",
    "\n",
    "        # Get mean loss and r2 score of epoch batches\n",
    "        losses.append(np.mean(losses_epoch))\n",
    "        r2_scores.append(np.mean(r2_scores_epoch))\n",
    "\n",
    "        # Clear the current figure\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot loss over epoch\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('loss')\n",
    "        plt.plot(np.arange(epoch + 1), np.array(losses), 'r-', label='loss')\n",
    "\n",
    "        # Plot R2 score over epoch\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('r2')\n",
    "        plt.plot(np.arange(epoch + 1), np.array(r2_scores), 'r-', label='r2')\n",
    "\n",
    "        plt.draw()\n",
    "    # wait for keyboard press to close tkAgg\n",
    "    while True:\n",
    "        if plt.waitforbuttonpress():\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
